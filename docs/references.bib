@article{abe2011,
  title = {Reward Improves Long-Term Retention of a Motor Memory through Induction of Offline Memory Gains},
  author = {Abe, Mitsunari and Schambra, Heidi and Wassermann, Eric M. and Luckenbaugh, Dave and Schweighofer, Nicolas and Cohen, Leonardo G.},
  date = {2011-04-12},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {21},
  number = {7},
  pages = {557--562},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2011.02.030},
  url = {https://www.sciencedirect.com/science/article/pii/S0960982211002211},
  urldate = {2022-07-19},
  abstract = {In humans, training in which good performance is rewarded or bad performance punished results in transient behavioral improvements [1, 2, 3]. The relative effects of reward and punishment on consolidation and long-term retention, critical behavioral stages for successful learning [4, 5], are not known. Here, we investigated the effects of reward and punishment on these different stages of human motor skill learning. We studied healthy subjects who trained on a motor task under rewarded, punished, or neutral control conditions. Performance was tested before and immediately, 6~hr, 24~hr, and 30~days after training in the absence of reward or punishment. Performance improvements immediately after training were comparable in the three groups. At 6~hr, the rewarded group maintained performance gains, whereas the other two groups experienced significant forgetting. At~24~hr, the reward group showed significant offline (posttraining) improvements, whereas the other two groups did not. At 30~days, the rewarded group retained the gains identified at 24~hr, whereas the other two groups experienced significant forgetting. We conclude that training under rewarded conditions is more effective than training under punished or neutral conditions in eliciting lasting motor learning, an advantage driven by offline memory gains that persist over time.},
  langid = {english},
}

@article{avila2012,
  title = {Positive Social-Comparative Feedback Enhances Motor Learning in Children},
  author = {Ávila, Luciana T. G. and Chiviacowsky, Suzete and Wulf, Gabriele and Lewthwaite, Rebecca},
  date = {2012-11-01},
  journaltitle = {Psychology of Sport and Exercise},
  shortjournal = {Psychology of Sport and Exercise},
  volume = {13},
  number = {6},
  pages = {849--853},
  issn = {1469-0292},
  doi = {10.1016/j.psychsport.2012.07.001},
  url = {https://www.sciencedirect.com/science/article/pii/S146902921200088X},
  urldate = {2022-07-19},
  abstract = {Objectives The present study investigated the influence of social-comparative feedback on the learning of a throwing task in 10-year-old children. Design Two-group experimental design, including a practice phase and retention test. Method Both groups of participants, a positive social-comparative feedback and a control group, received veridical feedback about their performance (accuracy score) after each practice trial. In addition, after each block of 10 trials, the positive feedback group was given bogus feedback suggesting that their own performance was better than that of a peer group's on that block. One day after the practice phase, a retention test without (veridical or social-comparative) feedback was performed to assess learning effects as a function of feedback. Results The positive feedback group demonstrated greater throwing accuracy than the control group on the retention test. In addition, questionnaire results indicated that this group scored higher in terms of perceived competence than the control group. Conclusions These findings demonstrate that feedback can have an important motivational function that affects the learning of motor skills in children.},
  langid = {english},
  keywords = {Perceived competence,Positive feedback,Throwing}
}

@article{bacelar2022,
  title = {Meta-Analyzing Enhanced Expectancies on Motor Learning: Positive Effects but Methodological Concerns},
  shorttitle = {Meta-Analyzing Enhanced Expectancies on Motor Learning},
  author = {Bacelar, Mariane F. B. and Parma, Juliana O. and Murrah, William M. and Miller, Matthew W.},
  year = {2022},
  journaltitle = {International Review of Sport and Exercise Psychology},
  volume = {0},
  number = {0},
  pages = {1--30},
  publisher = {{Routledge}},
  issn = {1750-984X},
  doi = {10.1080/1750984X.2022.2042839},
  url = {https://doi.org/10.1080/1750984X.2022.2042839},
  urldate = {2022-07-19},
  abstract = {The OPTIMAL theory of motor learning predicts enhanced expectancies facilitate learning. This meta-analysis investigated this prediction by quantifying effect sizes from studies manipulating feedback after good trials, comparative feedback, perceived task difficulty, conceptions of ability, self-modeling, or extrinsic rewards, and assessing learning. After searching databases, 48 studies met inclusion criteria for quantitative analysis. Fifty-six effect sizes were calculated from performance differences between enhanced expectancies and neutral/diminished expectancies groups at delayed retention tests. Results revealed a Hedges’ g = 0.54 (95\% CI [0.38, 0.69]) that may be overestimated due to small-study effects and underpowered studies. Moderator analyses revealed feedback after good trials, comparative feedback, perceived task difficulty, and conceptions of ability manipulations significantly improved learning. Few studies used other manipulations, precluding reliable estimates of their effects. Further, significant effects of enhanced expectancies were shown for children/adolescents, young adults, and older adults. Few studies examined special populations, preventing a reliable effect estimate. Finally, the effect of enhanced expectancies on learning was not affected by type of comparison group (neutral/diminished expectancies). Findings suggest enhanced expectancies may facilitate motor learning, consistent with OPTIMAL theory, but pre-registered/registered reports and more powerful studies should be conducted to confirm the effect and estimate its size more accurately.},
  keywords = {meta-analysis,motivation,motor skill,OPTIMAL theory,self-efficacy},
  annotation = {\_eprint: https://doi.org/10.1080/1750984X.2022.2042839}
}

@article{bacelar2022a,
  title = {Dissociating the Contributions of Motivational and Information Processing Factors to the Self-Controlled Feedback Learning Benefit},
  author = {Bacelar, Mariane F. B. and Parma, Juliana Otoni and Cabral, Daniel and Daou, Marcos and Lohse, Keith R. and Miller, Matthew W.},
  date = {2022-03-01},
  journaltitle = {Psychology of Sport and Exercise},
  shortjournal = {Psychology of Sport and Exercise},
  volume = {59},
  pages = {102119},
  issn = {1469-0292},
  doi = {10.1016/j.psychsport.2021.102119},
  url = {https://www.sciencedirect.com/science/article/pii/S1469029221002375},
  urldate = {2022-07-19},
  abstract = {Giving learners control over their feedback schedule has been shown to enhance motor learning. This effect has been attributed to enhanced intrinsic motivation via fulfilling learners’ needs for feelings of autonomy and competence, and greater information processing through provoking learners to estimate their errors. However, there is a lack of studies dissociating the contributions of motivational and information processing factors to the self-controlled feedback learning effect. To address this shortcoming, we crossed self-controlled feedback and error estimation in the same experimental design in the largest pre-registered self-control study to date (N~=~200). Participants performed a nondominant arm bean bag tossing task under one of four training conditions in which feedback schedule was either controlled by the participant or matched to a counterpart and error estimation was either mandatory or not enforced. Learning was assessed 24~h after the acquisition phase in retention and transfer tests. Results showed no statistically significant learning advantage for participants given control over feedback despite promoting spontaneous error estimation, and, surprisingly, results showed a disadvantage specific to the transfer test for participants obligated to estimate their errors. Further, although self-control over feedback resulted in its delivery on relatively accurate trials and slightly increased learners’ perceived autonomy, it did not enhance perceived competence or intrinsic motivation. At the individual level, however, intrinsic motivation did predict motor learning. The present study challenges the benefit of self-controlled feedback while supporting the positive effect of intrinsic motivation on motor learning.},
  langid = {english},
  keywords = {Autonomy support,Intrinsic motivation,OPTIMAL theory,Performance estimation}
}

@article{bartos2020,
  title = {Z-Curve 2.0: Estimating Replication Rates and Discovery Rates},
  shorttitle = {Z-{{Curve}}.2.0},
  author = {Bartoš, František and Schimmack, Ulrich},
  year = {2022},
  volume = {6},
  journaltitle = {Meta-Psychology},
  doi = {10.15626/MP.2021.2720},
  url = {https://doi.org/10.15626/MP.2021.2720},
  urldate = {2022-07-19},
  abstract = {This article introduces z-curve.2.0 as a method that estimates the expected replication rate (ERR) and the expected discovery rate (EDR) based on the test-statistics of studies selected for significance. Z-curve.2.0 extends the work by Brunner and Schimmack (2019) in several ways. First, we show that a new estimation method using expectation-maximization outperforms the kernel-density approach of z-curve.1.0. Second, we examine the coverage of bootstrapped confidence intervals to provide information about the uncertainty in z-curve estimates. Third, we extended z-curve to estimate the number of all studies that were conducted, including studies with non-significant results that may not have been reported, solely on the basis of significant results. This allows us to estimate the EDR; that is, the percentage of significant results that were obtained in all studies. EDR can be used to assess the size of the file-drawer, estimate the maximum number of false positive results, and may provide a better estimate of the success rate in actual replication studies than the ERR because exact replications are impossible.},
  langid = {american},
  keywords = {Expectancy Maximization,Expected Discovery Rate,Expected Replication Rate,False Discovery Rate,File-Drawer,Meta-science,Meta-Science,Post-hoc power analysis,Power estimation,Publication bias,Quantitative Methods,Replicability,Social and Behavioral Sciences,Statistical Methods,Z-curve},
}

@article{bartos2021,
  title = {Robust Bayesian meta-analysis: Model-averaging across complementary publication bias adjustment methods},
  shorttitle = {Robust {{Bayesian Meta-Analysis}}},
  author = {Bartoš, František and Maier, Maximilian and Wagenmakers, Eric-Jan and Doucouliagos, Hristos and Stanley, T. D.},
  year = {2022},
  publisher = {Wiley Online Library},
  journal = {Research Synthesis Methods},
  doi = {10.1002/jrsm.1594},
  abstract = {Publication bias is a ubiquitous threat to the validity of meta-analysis and the accumulation of scientific evidence. In order to estimate and counteract the impact of publication bias, multiple methods have been developed; however, recent simulation studies have shown the methods' performance to depend on the true data generating process, and no method consistently outperforms the others across a wide range of conditions. Unfortunately, when different methods lead to contradicting conclusions, researchers can choose those methods that lead to a desired outcome. To avoid the condition-dependent, all-or-none choice between competing methods and conflicting results, we extend robust Bayesian meta-analysis and model-average across two prominent approaches of adjusting for publication bias: (1) selection models of p-values and (2) models adjusting for small-study effects. The resulting model ensemble weights the estimates and the evidence for the absence/presence of the effect from the competing approaches with the support they receive from the data. Applications, simulations, and comparisons to preregistered, multi-lab replications demonstrate the benefits of Bayesian model-averaging of complementary publication bias adjustment methods.},
  langid = {american},
  pages = {1--18},
  keywords = {Bayesian Model-Averaging,Meta-Analysis,PET-PEESE,Publication Bias,Quantitative Methods,SelectionModels,Social and Behavioral Sciences,Statistical Methods}
}

@article{carter2019,
  title = {Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods},
  shorttitle = {Correcting for {{Bias}} in {{Psychology}}},
  author = {Carter, Evan C. and Schönbrodt, Felix D. and Gervais, Will M. and Hilgard, Joseph},
  date = {2019-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {115--144},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919847196},
  url = {https://doi.org/10.1177/2515245919847196},
  urldate = {2022-07-19},
  abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.},
  langid = {english},
  keywords = {bias correction,meta-analysis,open data,open materials,p-hacking,publication bias,questionable research practices}
}

@article{chiviacowsky2007,
  title = {Feedback After Good Trials Enhances Learning},
  author = {Chiviacowsky, Suzete and Wulf, Gabriele},
  date = {2007-03-01},
  journaltitle = {Research Quarterly for Exercise and Sport},
  volume = {78},
  number = {2},
  eprint = {17479573},
  eprinttype = {pmid},
  pages = {40--47},
  publisher = {{Routledge}},
  issn = {0270-1367},
  doi = {10.1080/02701367.2007.10599402},
  url = {https://doi.org/10.1080/02701367.2007.10599402},
  urldate = {2022-07-19},
  abstract = {Recent studies (Chiviacowsky \& Wulf, 2002, 2005) have shown that learners prefer to receive feedback after they believe they had a “good” rather than “poor”trial. The present study followed up on this finding and examined whether learning would benefit if individuals received feedback after good relative to poor trials. Participants practiced a task that required them to throw beanbags at a target with their nondominant arm. Vision was prevented during and after the throws. All participants received knowledge of results (KR) on three trials in each 6-trial block. While one group (KR good) received KR for the three most effective trials in each block, another (KR poor) received feedback for the three least effective trials in each block. There were no group differences in practice. However, the KR good group showed learning advantages on a delayed retention test (without KR). These results demonstrated that learning is facilitated if feedback is provided after good rather than poor trials. The findings are interpreted as evidence for a motivational function of feedback.},
  keywords = {guidance hypothesis,knowledge of results,motor learning,throwing},
  annotation = {\_eprint: https://doi.org/10.1080/02701367.2007.10599402}
}

@article{cimpian2007,
  title = {Subtle Linguistic Cues Affect Children's Motivation},
  author = {Cimpian, Andrei and Arce, Holly-Marie C. and Markman, Ellen M. and Dweck, Carol S.},
  date = {2007-04-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {18},
  number = {4},
  pages = {314--316},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.2007.01896.x},
  url = {https://doi.org/10.1111/j.1467-9280.2007.01896.x},
  urldate = {2022-07-19},
  langid = {english}
}

@article{dempster1977,
  title = {Maximum Likelihood from Incomplete Data via the {{EM}} Algorithm},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  date = {1977},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {39},
  number = {1},
  eprint = {2984875},
  eprinttype = {jstor},
  pages = {1--38},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
}

@article{duval1998,
  title = {Practical Estimates of the Effect of Publication Bias in Meta-Analysis},
  author = {Duval, S. and Tweedie, R.},
  date = {1998},
  journaltitle = {Australasian Epidemiologist},
  volume = {5},
  number = {4},
  publisher = {{Australasian Epidemiological Association}},
  doi = {10.3316/informit.434005834350149},
  url = {https://search.informit.org/doi/abs/10.3316/informit.434005834350149},
  urldate = {2022-07-19},
  abstract = {One of the major drawbacks to using meta-analysis is the possibility of 'publication bias'. The studies for a meta-analysis are usually chosen through a literature review and an inherent selection bias may thereby arise. This paper presents the 'trim and fill' method, a new technique developed for estimating and adjusting for the numbers and outcomes of missing studies. It has the advantage of being computationally simple and, in practical situations, to perform better than existing methods. To illustrate the trim and fill method, the authors apply it to an existing meta-analysis of the effect of using gangliosides in reducing case fatality in acute stroke. (non-author abstract)}
}

@article{fanelli2010,
  title = {Do Pressures to Publish Increase Scientists' Bias? An Empirical Support from US States Data},
  shorttitle = {Do {{Pressures}} to {{Publish Increase Scientists}}' {{Bias}}?},
  author = {Fanelli, Daniele},
  date = {2010-04-21},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {5},
  number = {4},
  pages = {e10271},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0010271},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010271},
  urldate = {2022-07-19},
  abstract = {The growing competition and “publish or perish” culture in academia might conflict with the objectivity and integrity of research, because it forces scientists to produce “publishable” results at all costs. Papers are less likely to be published and to be cited if they report “negative” results (results that fail to support the tested hypothesis). Therefore, if publication pressures increase scientific bias, the frequency of “positive” results in the literature should be higher in the more competitive and “productive” academic environments. This study verified this hypothesis by measuring the frequency of positive results in a large random sample of papers with a corresponding author based in the US. Across all disciplines, papers were more likely to support a tested hypothesis if their corresponding authors were working in states that, according to NSF data, produced more academic papers per capita. The size of this effect increased when controlling for state's per capita R\&D expenditure and for study characteristics that previous research showed to correlate with the frequency of positive results, including discipline and methodology. Although the confounding effect of institutions' prestige could not be excluded (researchers in the more productive universities could be the most clever and successful in their experiments), these results support the hypothesis that competitive academic environments increase not only scientists' productivity but also their bias. The same phenomenon might be observed in other countries where academic competition and pressures to publish are high.},
  langid = {english},
  keywords = {Behavioral neuroscience,Bibliometrics,Careers,Drug interactions,Scientific misconduct,Scientists,Toxicology,United States},
}

@article{gelman2014,
  title = {Beyond Power Calculations: {{Assessing Type S}} (Sign) and {{Type M}} (Magnitude) Errors},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  date = {2014-11-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {9},
  number = {6},
  pages = {641--651},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691614551642},
  url = {https://doi.org/10.1177/1745691614551642},
  urldate = {2022-07-19},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  langid = {english},
  keywords = {design calculation,exaggeration ratio,power analysis,replication crisis,statistical significance,Type M error,Type S error},
}

@article{guadagnoli2004,
  title = {Challenge Point: A Framework for Conceptualizing the Effects of Various Practice Conditions in Motor Learning},
  shorttitle = {Challenge {{Point}}},
  author = {Guadagnoli, Mark A. and Lee, Timothy D.},
  date = {2004-07-01},
  journaltitle = {Journal of Motor Behavior},
  volume = {36},
  number = {2},
  eprint = {15130871},
  eprinttype = {pmid},
  pages = {212--224},
  publisher = {{Routledge}},
  issn = {0022-2895},
  doi = {10.3200/JMBR.36.2.212-224},
  url = {https://doi.org/10.3200/JMBR.36.2.212-224},
  urldate = {2022-07-19},
  abstract = {The authors describe the effects of practice conditions in motor learning (e.g., contextual interference, knowledge of results) within the constraints of 2 experimental variables: skill level and task difficulty. They use a research framework to conceptualize the interaction of those variables on the basis of concepts from information theory and information processing. The fundamental idea is that motor tasks represent different challenges for performers of different abilities. The authors propose that learning is related to the information arising from performance, which should be optimized along functions relating the difficulty of the task to the skill level of the performer. Specific testable hypotheses arising from the framework are also described.},
  keywords = {augmented feedback,contextual interference,motor learning},
  annotation = {\_eprint: https://doi.org/10.3200/JMBR.36.2.212-224}
}

@article{hagger2016,
  title = {A Multilab Preregistered Replication of the Ego-Depletion Effect},
  author = {Hagger, M. S. and Chatzisarantis, N. L. D. and Alberts, H. and Anggono, C. O. and Batailler, C. and Birt, A. R. and Brand, R. and Brandt, M. J. and Brewer, G. and Bruyneel, S. and Calvillo, D. P. and Campbell, W. K. and Cannon, P. R. and Carlucci, M. and Carruth, N. P. and Cheung, T. and Crowell, A. and De Ridder, D. T. D. and Dewitte, S. and Elson, M. and Evans, J. R. and Fay, B. A. and Fennis, B. M. and Finley, A. and Francis, Z. and Heise, E. and Hoemann, H. and Inzlicht, M. and Koole, S. L. and Koppel, L. and Kroese, F. and Lange, F. and Lau, K. and Lynch, B. P. and Martijn, C. and Merckelbach, H. and Mills, N. V. and Michirev, A. and Miyake, A. and Mosser, A. E. and Muise, M. and Muller, D. and Muzi, M. and Nalis, D. and Nurwanti, R. and Otgaar, H. and Philipp, M. C. and Primoceri, P. and Rentzsch, K. and Ringos, L. and Schlinkert, C. and Schmeichel, B. J. and Schoch, S. F. and Schrama, M. and Schütz, A. and Stamos, A. and Tinghög, G. and Ullrich, J. and {vanDellen}, M. and Wimbarti, S. and Wolff, W. and Yusainy, C. and Zerhouni, O. and Zwienenberg, M.},
  date = {2016-07-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {11},
  number = {4},
  pages = {546--573},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691616652873},
  url = {https://doi.org/10.1177/1745691616652873},
  urldate = {2022-07-19},
  abstract = {Good self-control has been linked to adaptive outcomes such as better health, cohesive personal relationships, success in the workplace and at school, and less susceptibility to crime and addictions. In contrast, self-control failure is linked to maladaptive outcomes. Understanding the mechanisms by which self-control predicts behavior may assist in promoting better regulation and outcomes. A popular approach to understanding self-control is the strength or resource depletion model. Self-control is conceptualized as a limited resource that becomes depleted after a period of exertion resulting in self-control failure. The model has typically been tested using a sequential-task experimental paradigm, in which people completing an initial self-control task have reduced self-control capacity and poorer performance on a subsequent task, a state known as ego depletion. Although a meta-analysis of ego-depletion experiments found a medium-sized effect, subsequent meta-analyses have questioned the size and existence of the effect and identified instances of possible bias. The analyses served as a catalyst for the current Registered Replication Report of the ego-depletion effect. Multiple laboratories (k = 23, total N = 2,141) conducted replications of a standardized ego-depletion protocol based on a sequential-task paradigm by Sripada et al. Meta-analysis of the studies revealed that the size of the ego-depletion effect was small with 95\% confidence intervals (CIs) that encompassed zero (d = 0.04, 95\% CI [−0.07, 0.15]. We discuss implications of the findings for the ego-depletion effect and the resource depletion model of self-control.},
  langid = {english},
  keywords = {energy model,meta-analysis,resource depletion,self-regulation,strength model},
}

@article{lakens2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Daniël},
  date = {2014},
  journaltitle = {European Journal of Social Psychology},
  volume = {44},
  number = {7},
  pages = {701--710},
  issn = {1099-0992},
  doi = {10.1002/ejsp.2023},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2023},
  urldate = {2022-07-19},
  abstract = {Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre-registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null-hypothesis significance testing (NHST) are discussed. Sequential analyses, which are widely used in large-scale medical trials, provide an efficient way to perform high-powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research. Copyright © 2014 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ejsp.2023},
}

@article{lakens2022,
  title = {Sample Size Justification},
  author = {Lakens, Daniël},
  year = {2022},
  journaltitle = {Collabra: Psychology},
  shortjournal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  url = {https://doi.org/10.1525/collabra.33267},
  urldate = {2022-07-19},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
}

@misc{lakens2021,
  title = {Group Sequential Designs: A Tutorial},
  shorttitle = {Group {{Sequential Designs}}},
  author = {Lakens, Daniel and Pahlke, Friedrich and Wassmer, Gernot},
  date = {2021-01-27T07:12:19},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/x4azm},
  url = {https://psyarxiv.com/x4azm/},
  urldate = {2022-07-19},
  abstract = {This tutorial illustrates how to design, analyze, and report group sequential designs. In these designs, groups of observations are collected and repeatedly analyzed, while controlling error rates. Compared to a fixed sample size design, where data is analyzed only once, group sequential designs offer the possibility to stop the study at interim looks at the data either for efficacy or futility. Hence, they provide greater flexibility and are more efficient in the sense that due to early stopping the expected sample size is smaller as compared to the sample size in the design with no interim look. In this tutorial we illustrate how to use the R package 'rpact' and the associated Shiny app to design studies that control the Type I error rate when repeatedly analyzing data, even when neither the number of looks at the data, nor the exact timing of looks at the data, is specified. Specifically for *t*-tests, we illustrate how to perform an a-priori power analysis for group sequential designs, and explain how to stop the data collection for futility by rejecting the presence of an effect of interest based on a beta-spending function. Finally, we discuss how to report adjusted effect size estimates and confidence intervals. The recent availability of accessible software such as 'rpact' makes it possible for psychologists to benefit from the efficiency gains provided by group sequential designs.},
  langid = {american},
  keywords = {error control,Experimental Design and Sample Surveys,group sequential designs,hypothesis testing,power analysis,Quantitative Methods,sample size justification,sequential analyses,Social and Behavioral Sciences,Statistical Methods},
}

@article{lee2011,
  title = {Putting Like a Pro: The Role of Positive Contagion in Golf Performance and Perception},
  shorttitle = {Putting {{Like}} a {{Pro}}},
  author = {Lee, Charles and Linkenauger, Sally A. and Bakdash, Jonathan Z. and Joy-Gaba, Jennifer A. and Profitt, Dennis R.},
  date = {2011-10-20},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {6},
  number = {10},
  pages = {e26016},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0026016},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026016},
  urldate = {2022-07-19},
  abstract = {Many amateur athletes believe that using a professional athlete's equipment can improve their performance. Such equipment can be said to be affected with positive contagion, which refers to the belief of transference of beneficial properties between animate persons/objects to previously neutral objects. In this experiment, positive contagion was induced by telling participants in one group that a putter previously belonged to a professional golfer. The effect of positive contagion was examined for perception and performance in a golf putting task. Individuals who believed they were using the professional golfer's putter perceived the size of the golf hole to be larger than golfers without such a belief and also had better performance, sinking more putts. These results provide empirical support for anecdotes, which allege that using objects with positive contagion can improve performance, and further suggest perception can be modulated by positive contagion.},
  langid = {english},
  keywords = {Control theory,Equipment,Human performance,Intelligence tests,Personal computers,Research assessment,Sports,Surveys},
}

@article{lee2012,
  title = {{{EM}} Algorithms for Multivariate {{Gaussian}} Mixture Models with Truncated and Censored Data},
  author = {Lee, Gyemin and Scott, Clayton},
  date = {2012-09-01},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {56},
  number = {9},
  pages = {2816--2829},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2012.03.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167947312001156},
  urldate = {2022-07-19},
  abstract = {We present expectation–maximization (EM) algorithms for fitting multivariate Gaussian mixture models to data that are truncated, censored or truncated and censored. These two types of incomplete measurements are naturally handled together through their relation to the multivariate truncated Gaussian distribution. We illustrate our algorithms on synthetic and flow cytometry data.},
  langid = {english},
  keywords = {Censoring,EM algorithm,Multivariate Gaussian mixture model,Multivariate truncated Gaussian distribution,Truncation},
}

@article{lee2021,
  title = {Motor Learning: Reflections on the Past 40 Years of Research},
  shorttitle = {Motor {{Learning}}},
  author = {Lee, Timothy D. and Carnahan, Heather},
  date = {2021-06-20},
  journaltitle = {Kinesiology Review},
  volume = {10},
  number = {3},
  pages = {274--282},
  publisher = {{Human Kinetics}},
  issn = {2161-6035, 2163-0453},
  doi = {10.1123/kr.2021-0018},
  url = {https://journals.humankinetics.com/view/journals/krj/10/3/article-p274.xml},
  urldate = {2022-07-19},
  abstract = {The authors reflect on the dire state of motor learning at the time of Brooks’s book and consider reasons why research was resurrected in the 1980s and flourished in the ensuing years. In so doing, they provide an overview of the various research topics that have been studied, discuss the influence of motor learning on other fields of study, and consider the future of motor learning research both within and outside the academic study of kinesiology.},
  langid = {english}
}

@article{lewthwaite2015,
  title = {Choose to Move: The Motivational Impact of Autonomy Support on Motor Learning},
  shorttitle = {Choose to Move},
  author = {Lewthwaite, Rebecca and Chiviacowsky, Suzete and Drews, Ricardo and Wulf, Gabriele},
  date = {2015-10-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {22},
  number = {5},
  pages = {1383--1388},
  issn = {1531-5320},
  doi = {10.3758/s13423-015-0814-7},
  url = {https://doi.org/10.3758/s13423-015-0814-7},
  urldate = {2022-07-19},
  abstract = {Numerous studies in the motor learning domain have demonstrated learning advantages of self-controlled practice relative to yoked conditions. In separate lines of evidence in the social-psychological literature, findings show that providing participants with task-relevant autonomy support or minor incidental choices can result in superior outcomes when compared with conditions that thwart autonomy or do not offer choice. We hypothesized that motor learning could be enhanced by providing learners with choices – even if those choices are unrelated to task performance. In Experiment 1, two groups of participants practiced a golf putting task. While one group (the choice group) was able to select the color of golf balls (white, yellow, or orange) to be used in each upcoming block of 10 trials, participants in the second group (the yoked group) were provided with the same colored golf balls their choice-group counterparts had chosen. The results of a 24-h delayed retention test indicated significantly greater putting accuracy for the choice compared with the yoked group. Experiment 2 went one step further by asking choice group participants for their preferences regarding two issues unrelated to the practice task (balancing on a stabilometer): (1) which of two subsequent tasks (coincident timing or hand dynamometry) they wanted to perform and (2) which of two prints of paintings by Renoir they thought the investigator should hang on the laboratory wall. Yoked group participants were simply informed about which task they would perform afterwards and of which painting the experimenter would put on the wall. Balance learning was significantly more effective in the choice group on a retention test. Thus, self-controlled practice conditions can influence motor learning without providing task-relevant information, content, or strategic learning advantages. Self-controlled effects in motor learning may be motivational in nature, attributable to satisfaction of fundamental autonomy needs.},
  langid = {english},
  keywords = {Balance learning,Choice,Fundamental psychological needs,Golf putting,Self-controlled practice},
}

@article{lohse2016,
  title = {Underpowered and Overworked: Problems With Data Analysis in Motor Learning Studies},
  shorttitle = {Underpowered and {{Overworked}}},
  author = {Lohse, Keith and Buchanan, Taylor and Miller, Matthew},
  date = {2016-06-01},
  journaltitle = {Journal of Motor Learning and Development},
  volume = {4},
  number = {1},
  pages = {37--58},
  publisher = {{Human Kinetics, Inc.}},
  issn = {2325-3193, 2325-3215},
  doi = {10.1123/jmld.2015-0010},
  url = {https://journals.humankinetics.com/view/journals/jmld/4/1/article-p37.xml},
  urldate = {2022-07-19},
  abstract = {Appropriate statistical analysis is essential for accurate and reliable research. Statistical practices have an immediate impact on the perceived results of a single study but also remote effects on the dissemination of information among scientists and the cumulative nature of research. To accurately quantify potential problems facing the field of motor learning, we systematically reviewed publications from seven journals over the past 2 years to find experiments that tested the effects of different training conditions on delayed retention and transfer tests (i.e., classic motor learning paradigms). Eighteen studies were included. These studies had small sample sizes (Mdn n/group = 11.00, interquartile range [IQR]= 9.6–15.5), multiple dependent variables (Mdn = 2, IQR = 2–4), and many statistical tests per article (Mdn = 83.5, IQR = 55.8–112.5). The observed effect sizes were large (d = 0.71, IQR = 0.49, 1.11). However, the distribution of effect sizes was biased, t(16) = 3.48, p {$<$} .01. These metadata indicate problems with the way motor learning research is conducted (or at least published). We recommend several potential solutions to address these issues: a priori power calculations, prespecified analyses, data sharing, and dissemination of null results. Furthermore, we hope these data will spark serious action from all stakeholders (researchers, editorial boards, and publishers) in the field.},
  langid = {american}
}

@article{maier2022,
  title = {Robust {{Bayesian}} Meta-Analysis: Addressing Publication Bias with Model-Averaging},
  shorttitle = {Robust {{Bayesian}} Meta-Analysis},
  author = {Maier, Maximilian and Bartoš, František and Wagenmakers, Eric-Jan},
  year = {2022},
  journaltitle = {Psychological Methods},
  publisher = {{American Psychological Association}},
  location = {{Washington, US}},
  issn = {1082-989X},
  doi = {10.1037/met0000405},
  url = {https://doi.org/10.1037/met0000405},
  urldate = {2022-07-19},
  abstract = {Meta-analysis is an important quantitative tool for cumulative science, but its application is frustrated by publication bias. In order to test and adjust for publication bias, we extend model-averaged Bayesian meta-analysis with selection models. The resulting robust Bayesian meta-analysis (RoBMA) methodology does not require all-or-none decisions about the presence of publication bias, can quantify evidence in favor of the absence of publication bias, and performs well under high heterogeneity. By model-averaging over a set of 12 models, RoBMA is relatively robust to model misspecification and simulations show that it outperforms existing methods. We demonstrate that RoBMA finds evidence for the absence of publication bias in Registered Replication Reports and reliably avoids false positives. We provide an implementation in R so that researchers can easily use the new methodology in practice. (PsycInfo Database Record (c) 2022 APA, all rights reserved) (Source: journal abstract) Meta-analysis is an essential tool to synthesize information from a series of primary studies. However, the application of meta-analysis is often frustrated by publication bias—the fact that statistically significant results are published more often than nonsignificant results. To alleviate the problem of publication bias we developed a robust Bayesian meta-analysis (RoBMA). RoBMA applies a series of meta-analytic models to the data simultaneously and estimates the effect size by taking all models into account. RoBMA can quantify evidence for the presence as well as the absence of publication bias, RoBMA can correct for publication bias in cases where the true effect effect size differs between studies, and RoBMA does not require all-or-none decisions. We illustrate RoBMA with a meta-analysis on violent video games and aggressive behavior, and apply it to the ManyLabs 2 data for which we know publication bias to be absent. Simulations suggest that RoBMA provides a valuable complement to current methods for meta-analysis. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  langid = {english},
  keywords = {Adjustment,Bayesian Analysis (major),Errors (major),Meta Analysis (major),Models (major),Sciences,Simulation,Statistical Probability (major)},
  annotation = {(US)},
}

@article{mathur2020,
  title = {Sensitivity Analysis for Publication Bias in Meta-Analyses},
  author = {Mathur, Maya B. and VanderWeele, Tyler J.},
  date = {2020},
  journaltitle = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {69},
  number = {5},
  pages = {1091--1119},
  issn = {1467-9876},
  doi = {10.1111/rssc.12440},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12440},
  urldate = {2022-07-19},
  abstract = {We propose sensitivity analyses for publication bias in meta-analyses. We consider a publication process such that ‘statistically significant’ results are more likely to be published than negative or “non-significant” results by an unknown ratio, η. Our proposed methods also accommodate some plausible forms of selection based on a study's standard error. Using inverse probability weighting and robust estimation that accommodates non-normal population effects, small meta-analyses, and clustering, we develop sensitivity analyses that enable statements such as ‘For publication bias to shift the observed point estimate to the null, “significant” results would need to be at least 30 fold more likely to be published than negative or “non-significant” results’. Comparable statements can be made regarding shifting to a chosen non-null value or shifting the confidence interval. To aid interpretation, we describe empirical benchmarks for plausible values of η across disciplines. We show that a worst-case meta-analytic point estimate for maximal publication bias under the selection model can be obtained simply by conducting a standard meta-analysis of only the negative and ‘non-significant’ studies; this method sometimes indicates that no amount of such publication bias could ‘explain away’ the results. We illustrate the proposed methods by using real meta-analyses and provide an R package: PublicationBias.},
  langid = {english},
  keywords = {File drawer,Meta-analysis,Publication bias,Sensitivity analysis},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssc.12440},
}

@article{maxwell1991,
  title = {A Comparison of Methods for Increasing Power in Randomized Between-Subjects Designs},
  author = {Maxwell, Scott E. and Cole, David A. and Arvey, Richard D. and Salas, Eduardo},
  date = {1991-09},
  journaltitle = {Psychological Bulletin},
  volume = {110},
  number = {2},
  pages = {328--337},
  publisher = {{American Psychological Association}},
  issn = {0033-2909},
  doi = {10.1037/0033-2909.110.2.328},
  url = {http://www.proquest.com/docview/614339978/abstract/4D1B9E809421455APQ/1},
  urldate = {2022-07-19},
  abstract = {Power in a randomized between-Ss design can be enhanced by increasing sample size or increasing alpha level. This article compares 2 other methods of increasing power by reducing within-group error: (1) by adding a pretest to the design and using analysis of covariance (ANCOVA) and (2) by increasing the length of the posttest and using analysis of variance (ANOVA). Results showed that the relative power of these approaches depends on the degree to which the posttest is lengthened, on the reliability of the posttest, and on the pretest–posttest correlation. When reliability or the pretest–posttest correlation is low, doubling the length of the posttest makes ANOVA more powerful than ANCOVA conducted on the original measures. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  langid = {english},
  pagetotal = {328-337},
  keywords = {Analysis of Covariance,Analysis of Variance,Between Groups Design (major),Posttesting (major),Pretesting (major),Statistical Analysis (major),Statistical Power (major)},
  annotation = {(US)},
}

@article{mckay2021,
  title = {Meta-Analytic Findings in the Self-Controlled Motor Learning Literature: Underpowered, Biased, and Lacking Evidential Value},
  shorttitle = {Meta-{{Analytic Findings}} in the {{Self-Controlled Motor Learning Literature}}},
  author = {McKay, Brad and Yantha, Zachary D. and Hussien, Julia and Carter, Michael J. and Ste-Marie, Diane M.},
  year = {2022},
  journaltitle = {Meta-Psychology},
  doi = {10.15626/MP.2021.2803},
  url = {https://doi.org/10.15626/MP.2021.2803},
  abstract = {The self-controlled motor learning literature consists of experiments that compare a group of learners who are provided with a choice over an aspect of their practice environment to a group who are yoked to those choices. A qualitative review of the literature suggests an unambiguous benefit from self-controlled practice. A meta-analysis was conducted on the effects of self-controlled practice on retention test performance measures with a focus on assessing and potentially correcting for selection bias in the literature, such as publication bias and p-hacking. First, a naïve random effects model was fit to the data and a moderate benefit of self-controlled practice, g=.44 (k= 52, N= 2061, 95\%CI [.31, .56]), was found. Second, publication status was added to the model as a potential moderator, revealing a significant difference between published and unpublished findings, with only the former reporting a benefit of self-controlled practice. Third, to investigate and adjust for the impact of selectively reporting statistically significant results, a weight-function model was fit to the data with a one-tailed p-value cutpoint of .025. The weight-function model revealed substantial selection bias and estimated the true average effect of self-controlled practice as g=.107 (95\%CI [.047, .18]). P-curve analyses were conducted on the statistically significant results published in the literature and the outcome suggested a lack of evidential value. Fourth, a suite of sensitivity analyses were conducted to evaluate the robustness of these results, all of which converged on trivially small effect estimates. Overall, our results suggest the benefit of self-controlled practice on motor learning is small and not currently distinguishable from zero.},
  langid = {american},
  keywords = {Choice,Meta-analysis,Meta-science,Motor learning,OPTIMAL Theory,p-curve,publication bias,Retention},
}

@article{mckay2022,
  title = {Autonomy Support via Instructionally Irrelevant Choice Not Beneficial for Motor Performance or Learning},
  author = {McKay, Brad and Ste-Marie, Diane M.},
  date = {2022-01-02},
  journaltitle = {Research Quarterly for Exercise and Sport},
  volume = {93},
  number = {1},
  eprint = {32854605},
  eprinttype = {pmid},
  pages = {64--76},
  publisher = {{Routledge}},
  issn = {0270-1367},
  doi = {10.1080/02701367.2020.1795056},
  url = {https://doi.org/10.1080/02701367.2020.1795056},
  urldate = {2022-07-19},
  abstract = {Purpose: The Optimizing Performance Through Motivation and Attention for Learning (OPTIMAL) theory predicts that providing learners with choices during skill acquisition will enhance their acquisition performance, motor learning, and expectancies. Based on this theory, it is recommended that instructors ask learners to choose which tasks to practice in applied settings. This experiment tested these predictions and recommendation by crossing autonomy support with practice schedule in a 2 × 2 factorial design. Method: Participants (N = 128) practiced a novel non-dominant hand dart-throwing task either with choice over the color of the dart flights (autonomy) or yoked to a counterpart’s choices (yoked). Further, participants either practiced throwing darts to three different targets in equal amounts (variable) or throwing to the same target for all practice trials (constant). All participants completed a pretest, acquisition phase, 24-hr delayed retention and transfer tests, as well as baseline and post-acquisition autonomy, and self-efficacy measures. Data were analyzed according to a pre-registered analysis plan that included pretest and gender as covariates. Results: The autonomy groups reported significantly greater perceived autonomy at the end of acquisition. There were no significant effects of autonomy on self-efficacy, or motor performance uniquely during acquisition, or uniquely on the delayed transfer test. The autonomy groups, however, performed with significantly greater error across acquisition and transfer. Practice schedule interacted with the time of testing such that the constant groups performed significantly more accurately during acquisition but non-significantly less accurately during transfer than the variable groups. Conclusions: These results are inconsistent with OPTIMAL theory.},
  keywords = {Autonomy,OPTIMAL theory,schema theory,variable practice},
  annotation = {\_eprint: https://doi.org/10.1080/02701367.2020.1795056},
}

@article{mcshane2016,
  title = {Adjusting for Publication Bias in Meta-Analysis: An Evaluation of Selection Methods and Some Cautionary Notes},
  shorttitle = {Adjusting for {{Publication Bias}} in {{Meta-Analysis}}},
  author = {McShane, Blakeley B. and Böckenholt, Ulf and Hansen, Karsten T.},
  date = {2016-09-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {11},
  number = {5},
  pages = {730--749},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691616662243},
  url = {https://doi.org/10.1177/1745691616662243},
  urldate = {2022-07-19},
  abstract = {We review and evaluate selection methods, a prominent class of techniques first proposed by Hedges (1984) that assess and adjust for publication bias in meta-analysis, via an extensive simulation study. Our simulation covers both restrictive settings as well as more realistic settings and proceeds across multiple metrics that assess different aspects of model performance. This evaluation is timely in light of two recently proposed approaches, the so-called p-curve and p-uniform approaches, that can be viewed as alternative implementations of the original Hedges selection method approach. We find that the p-curve and p-uniform approaches perform reasonably well but not as well as the original Hedges approach in the restrictive setting for which all three were designed. We also find they perform poorly in more realistic settings, whereas variants of the Hedges approach perform well. We conclude by urging caution in the application of selection methods: Given the idealistic model assumptions underlying selection methods and the sensitivity of population average effect size estimates to them, we advocate that selection methods should be used less for obtaining a single estimate that purports to adjust for publication bias ex post and more for sensitivity analysis—that is, exploring the range of estimates that result from assuming different forms of and severity of publication bias.},
  langid = {english},
  keywords = {effect size,meta-analysis,p-curve,p-uniform,selection methods},
}

@article{munafo2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  date = {2017-01-10},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  url = {http://www.nature.com/articles/s41562-016-0021},
  urldate = {2022-07-19},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  issue = {1},
  langid = {english},
  keywords = {Social sciences},
}

@article{osc2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {Collaboration, Open Science},
  date = {2015},
  journaltitle = {Science},
  volume = {349},
  number = {6251},
  eprint = {24749235},
  eprinttype = {jstor},
  pages = {943--943},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
}

@article{salmoni1984,
  title = {Knowledge of Results and Motor Learning: A Review and Critical Reappraisal},
  shorttitle = {Knowledge of Results and Motor Learning},
  author = {Salmoni, Alan W. and Schmidt, Richard A. and Walter, Charles B.},
  date = {1984-05},
  journaltitle = {Psychological Bulletin},
  volume = {95},
  number = {3},
  pages = {355--386},
  publisher = {{American Psychological Association}},
  issn = {0033-2909},
  doi = {10.1037/0033-2909.95.3.355},
  url = {http://www.proquest.com/docview/614319093/abstract/A66DB5F535D1499CPQ/1},
  urldate = {2022-07-19},
  abstract = {Examines some critical definitional and experimental-design problems that underlie the principles of knowledge of results (KR) and learning, the KR literature, and how newer principles of KR lead to notions of how KR works in human motor-learning situations. KR is defined as augmented feedback, where the KR is additional to those sources of feedback that are naturally received when a response is made. Transfer tests, usually under no-KR conditions, are essential for unraveling the temporary effects of KR manipulations from their relatively permanent learning effects. When this is considered, the literature reveals findings that produce reasonable agreement, although there are a number of inconsistencies in studies examining the same variables. When learning vs performance effects of KR are separated, a number of contradictions occur; new principles that emerge include the notion that KR acts as guidance, that it is motivating or energizing, and that it has a role in the formation of associations. It is suggested that KR may guide an S to the proper target behavior, with other processes (e.g., simple repetition) being the critical determinants of learning. (4 p ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  langid = {english},
  pagetotal = {355-386},
  keywords = {Knowledge of Results (major),Literature Review (major),Perceptual Motor Learning (major)},
  annotation = {(US)},
}

@article{sanli2013,
  title = {Understanding Self-Controlled Motor Learning Protocols through the Self-Determination Theory},
  author = {Sanli, Elizabeth and Patterson, Jae and Bray, Steven and Lee, Timothy},
  date = {2013},
  journaltitle = {Frontiers in Psychology},
  volume = {3},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00611},
  urldate = {2022-07-19},
  abstract = {The purpose of the present review was to provide a theoretical understanding of the learning advantages underlying a self-controlled practice context through the tenets of the self-determination theory (SDT). Three micro-theories within the macro-theory of SDT (Basic psychological needs theory, Cognitive Evaluation Theory, and Organismic Integration Theory) are used as a framework for examining the current self-controlled motor learning literature. A review of 26 peer-reviewed, empirical studies from the motor learning and medical training literature revealed an important limitation of the self-controlled research in motor learning: that the effects of motivation have been assumed rather than quantified. The SDT offers a basis from which to include measurements of motivation into explanations of changes in behavior. This review suggests that a self-controlled practice context can facilitate such factors as feelings of autonomy and competence of the learner, thereby supporting the psychological needs of the learner, leading to long term changes to behavior. Possible tools for the measurement of motivation and regulation in future studies are discussed. The SDT not only allows for a theoretical reinterpretation of the extant motor learning research supporting self-control as a learning variable, but also can help to better understand and measure the changes occurring between the practice environment and the observed behavioral outcomes.},
}

@article{schmidt1975,
  title = {A Schema Theory of Discrete Motor Skill Learning},
  author = {Schmidt, Richard A.},
  date = {1975-07},
  journaltitle = {Psychological Review},
  volume = {82},
  number = {4},
  pages = {225--260},
  publisher = {{American Psychological Association}},
  location = {{Washington, US}},
  issn = {0033-295X},
  doi = {10.1037/h0076770},
  url = {http://www.proquest.com/docview/614290235/abstract/C8F94960B5084C17PQ/1},
  urldate = {2022-07-19},
  abstract = {Argues that although a number of closed-loop postulations to explain motor skills learning and performance phenomena have appeared recently, each of these views suffers from either (a) logical problems in explaining the phenomena or (b) predictions that are not supported by the empirical evidence. After these difficulties are discussed, a new theory for discrete motor learning is proposed that is considered to be capable of explaining the existing findings. The theory is based on the notion of the schema and uses a recall memory to produce movement and a recognition memory to evaluate response correctness. Some of the predictions are mentioned, research techniques and paradigms that can be used to test the predictions are listed, and data in support of the theory are presented. (89 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  langid = {english},
  pagetotal = {225-260},
  keywords = {Learning Theory (major),Motor Performance (major),Motor Skills (major),Perceptual Motor Learning (major)},
  annotation = {(US)},
}

@article{simonsohn2015,
  title = {Small Telescopes: Detectability and the Evaluation of Replication Results},
  shorttitle = {Small {{Telescopes}}},
  author = {Simonsohn, Uri},
  date = {2015-05-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {26},
  number = {5},
  pages = {559--569},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797614567341},
  url = {https://doi.org/10.1177/0956797614567341},
  urldate = {2022-07-19},
  abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating “unsuccessful” replication attempts (i.e., studies yielding p {$>$} .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) “protecting” true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
  langid = {english},
  keywords = {hypothesis testing,open materials,replication,statistical power},
}

@misc{stefan2022,
  title = {Big Little Lies: A Compendium and Simulation of p-Hacking Strategies},
  shorttitle = {Big {{Little Lies}}},
  author = {Stefan, Angelika and Schönbrodt, Felix},
  date = {2022-03-16T17:56:36},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/xy2dk},
  url = {https://psyarxiv.com/xy2dk/},
  urldate = {2022-07-19},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of twelve p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
  langid = {american},
  keywords = {Error Rates,False Positive Results,Meta-science,p-Curve,Quantitative Methods,Questionable Research Practices,Shiny App,Significance,Simulation,Social and Behavioral Sciences,Statistical Methods},
}

@article{stemarie2011,
  title = {Feedforward Self-Modeling Enhances Skill Acquisition in Children Learning Trampoline Skills},
  author = {Ste-Marie, Diane and Vertes, Kelly and Rymal, Amanda and Martini, Rose},
  date = {2011},
  journaltitle = {Frontiers in Psychology},
  volume = {2},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2011.00155},
  urldate = {2022-07-19},
  abstract = {The purpose of this research was to examine whether children would benefit from a feedforward self-modeling (FSM) video and to explore possible explanatory mechanisms for the potential benefits, using a self-regulation framework. To this end, children were involved in learning two five-skill trampoline routines. For one of the routines, a FSM video was provided during acquisition, whereas only verbal instructions were provided for the alternate routine. The FSM involved editing video footage such that it showed the learner performing the trampoline routine at a higher skill level than their current capability. Analyses of the data showed that while physical performance benefits were observed for the routine that was learned with the FSM video, no differences were obtained in relation to the self-regulatory measures. Thus, the FSM video enhanced motor skill acquisition, but this could not be explained by changes to the varied self-regulatory processes examined.},
}

@article{sterne2000,
  title = {Publication and Related Bias in Meta-Analysis: Power of Statistical Tests and Prevalence in the Literature},
  shorttitle = {Publication and Related Bias in Meta-Analysis},
  author = {Sterne, Jonathan A. C and Gavaghan, David and Egger, Matthias},
  date = {2000-11-01},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {Journal of Clinical Epidemiology},
  volume = {53},
  number = {11},
  pages = {1119--1129},
  issn = {0895-4356},
  doi = {10.1016/S0895-4356(00)00242-0},
  url = {https://www.sciencedirect.com/science/article/pii/S0895435600002420},
  urldate = {2022-07-19},
  abstract = {Publication and selection biases in meta-analysis are more likely to affect small studies, which also tend to be of lower methodological quality. This may lead to “small-study effects,” where the smaller studies in a meta-analysis show larger treatment effects. Small-study effects may also arise because of between-trial heterogeneity. Statistical tests for small-study effects have been proposed, but their validity has been questioned. A set of typical meta-analyses containing 5, 10, 20, and 30 trials was defined based on the characteristics of 78 published meta-analyses identified in a hand search of eight journals from 1993 to 1997. Simulations were performed to assess the power of a weighted regression method and a rank correlation test in the presence of no bias, moderate bias or severe bias. We based evidence of small-study effects on P {$<$} 0.1. The power to detect bias increased with increasing numbers of trials. The rank correlation test was less powerful than the regression method. For example, assuming a control group event rate of 20\% and no treatment effect, moderate bias was detected with the regression test in 13.7\%, 23.5\%, 40.1\% and 51.6\% of meta-analyses with 5, 10, 20 and 30 trials. The corresponding figures for the correlation test were 8.5\%, 14.7\%, 20.4\% and 26.0\%, respectively. Severe bias was detected with the regression method in 23.5\%, 56.1\%, 88.3\% and 95.9\% of meta-anlyses with 5, 10, 20 and 30 trials, as compared to 11.9\%, 31.1\%, 45.3\% and 65.4\% with the correlation test. Similar results were obtained in simulations incorporating moderate treatment effects. However the regression method gave false-positive rates which were too high in some situations (large treatment effects, or few events per trial, or all trials of similar sizes). Using the regression method, evidence of small-study effects was present in 21 (26.9\%) of the 78 published meta-analyses. Tests for small-study effects should routinely be performed in meta-analysis. Their power is however limited, particularly for moderate amounts of bias or meta-analyses based on a small number of small studies. When evidence of small-study effects is found, careful consideration should be given to possible explanations for these in the reporting of the meta-analysis.},
  langid = {english},
  keywords = {Correlation,Funnel plot,Meta-analysis,Publication bias,Regression,Simulation study},
}

@article{stgermain2022,
  title = {Increased Perceptions of Autonomy through Choice Fail to Enhance Motor Skill Retention},
  author = {St. Germain, Laura and Williams, Allison and Balbaa, Noura and Poskus, Andrew and Lohse, Keith R. and Carter, Michael J.},
  date = {2022-04},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {48},
  number = {4},
  pages = {370--379},
  publisher = {{American Psychological Association}},
  location = {{Washington, US}},
  issn = {0096-1523},
  doi = {10.1037/xhp0000992},
  url = {http://www.proquest.com/docview/2632232277/abstract/13A75938B6CD4624PQ/1},
  urldate = {2022-07-19},
  abstract = {There has been growing research interest in the effects that motivation plays in motor learning, and specifically how autonomy, competence, and social relatedness may directly benefit the learning process. Here, we present a preregistered manipulation of autonomy-support by providing learners with choice during the practice of a speed cup-stacking task. One group was given control over when a video demonstration was provided and the viewing speed. A yoked control group received an identical demonstration schedule, but without choice (as their schedule was matched to a participant with choice). Critically, we addressed a gap in the literature by adding a yoked group who was explicitly told that they were being denied choice and that their schedule was chosen by another participant. We found no statistically significant learning differences between groups, despite finding evidence that providing choice increased perceived autonomy. Equivalence tests further showed that although the groups were not statistically equivalent, the effect size is likely too small to practically study the effects of autonomy-support through choice in most motor learning labs. These findings add to a growing body of research that questions a causal role of autonomy-support on motor learning, and the robustness of the so-called self-controlled learning advantage. (PsycInfo Database Record (c) 2022 APA, all rights reserved) (Source: journal abstract) The present study suggests that although university-aged students may experience a boost in feelings of autonomy when given choice over features of their learning environment, this boost may not enhance learning of a new motor skill. Our results also highlight the need for higher powered and preregistered motor learning experiments if coaches and practitioners are to be provided with reliable recommendations for training and rehabilitation protocols. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  langid = {english},
  pagetotal = {370-379},
  keywords = {Autonomy (major),Choice Shift (major),Learning,Measurement,Motivation,Motor Development (major),Motor Skills (major),Perception,Retention,Velocity (major)},
  annotation = {(US)},
}

@article{sutton2000,
  title = {Modelling Publication Bias in Meta-Analysis: A Review},
  shorttitle = {Modelling Publication Bias in Meta-Analysis},
  author = {Sutton, Alexander and Song, Fujian and Gilbody, Simon and Abrams, Keith},
  date = {2000},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Statistical Methods in Medical Research},
  volume = {9},
  number = {5},
  pages = {421--445},
  publisher = {{Sage Publications}},
  issn = {0962-2802},
  doi = {10.1177/096228020000900503},
  url = {https://journals.scholarsportal.info/details/09622802/v09i0005/421_mpbimar.xml},
  urldate = {2022-07-19},
  abstract = {Meta-analysis is now a widely used technique for summarizing evidence from multiple studies. Publication bias, the bias induced by the fact that research with statistically significant results is potentially more likely to be submitted and published than work with null or non-significant results, poses a thereat to the validity of such analyses. The implication of this is that combining only the identified published studies uncritically may lead to an incorrect, usually over optimistic, conclusion. How publication bias should be addressed when carrying out a meta-analysis is currently a hotly debated subject. While statistical methods to test for its presence are starting be used, they do not address the problem of how to proceed if publication bias is suspected. This paper provides a review of methods, which can be employed as a sensitivity analysis to assess the likely impact of publication bias on a meta-analysis. It is hoped that this will raise awareness of such methods, and promote their use and development, as well as provide an agenda for future research.},
}

@article{thornton2000,
  title = {Publication Bias in Meta-Analysis: Its Causes and Consequences},
  shorttitle = {Publication Bias in Meta-Analysis},
  author = {Thornton, Alison and Lee, Peter},
  date = {2000-02-01},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {Journal of Clinical Epidemiology},
  volume = {53},
  number = {2},
  pages = {207--216},
  issn = {0895-4356},
  doi = {10.1016/S0895-4356(99)00161-4},
  url = {https://www.sciencedirect.com/science/article/pii/S0895435699001614},
  urldate = {2022-07-19},
  abstract = {Publication bias is a widespread problem that may seriously distort attempts to estimate the effect under investigation. The literature is reviewed to determine features of the design and execution of both single studies and meta-analyses leading to publication bias, and the role the author, journal editor, and reviewer play in selecting studies for publication. Methods of detecting, correcting for, and preventing publication bias are reviewed. The design of the meta-analysis itself, and the studies included in it, are shown to be important among a number of sources of publication bias. Various factors influence an author's decision to submit results for publication. Journal editors and reviewers are crucial in deciding which studies to publish. Various methods proposed for detecting and correcting for publication bias, though useful, all have limitations. However, prevention of publication bias by registering every trial undertaken or publishing all studies is an ideal that is hard to achieve.},
  langid = {english},
  keywords = {Publication bias},
}

@article{vevea2005,
  title = {Publication Bias in Research Synthesis: Sensitivity Analysis Using A Priori Weight Functions},
  shorttitle = {Publication {{Bias}} in {{Research Synthesis}}},
  author = {Vevea, Jack L. and Woods, Carol M.},
  date = {2005-12},
  journaltitle = {Psychological Methods},
  volume = {10},
  number = {4},
  pages = {428--443},
  publisher = {{American Psychological Association}},
  location = {{Washington, US}},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.10.4.428},
  url = {http://www.proquest.com/docview/614438059/abstract/DD141A5A612B436BPQ/1},
  urldate = {2022-07-19},
  abstract = {Publication bias, sometimes known as the "file-drawer problem" or "funnel-plot asymmetry," is common in empirical research. The authors review the implications of publication bias for quantitative research synthesis (meta-analysis) and describe existing techniques for detecting and correcting it. A new approach is proposed that is suitable for application to meta-analytic data sets that are too small for the application of existing methods. The model estimates parameters relevant to fixed-effects, mixed-effects or random-effects meta-analysis contingent on a hypothetical pattern of bias that is fixed independently of the data. The authors illustrate this approach for sensitivity analysis using 3 data sets adapted from a commonly cited reference work on research synthesis (H. M. Cooper \& L. V. Hedges, 1994). (PsycINFO Database Record (c) 2019 APA, all rights reserved) (Source: journal abstract)},
  langid = {english},
  pagetotal = {428-443},
  keywords = {Effect Size (Statistical) (major),Experimentation (major),Meta Analysis,Quantitative Methods,Scientific Communication (major)},
  annotation = {(US)},
}

@article{vickers2001,
  title = {Analysing Controlled Trials with Baseline and Follow up Measurements},
  author = {Vickers, Andrew J. and Altman, Douglas G.},
  date = {2001-11-10},
  journaltitle = {BMJ},
  shortjournal = {BMJ},
  volume = {323},
  number = {7321},
  eprint = {11701584},
  eprinttype = {pmid},
  pages = {1123--1124},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.323.7321.1123},
  url = {http://www.bmj.com/content/323/7321/1123},
  urldate = {2022-07-19},
  abstract = {In many randomised trials researchers measure a continuous variable at baseline and again as an outcome assessed at follow up. Baseline measurements are common in trials of chronic conditions where researchers want to see whether a treatment can reduce pre-existing levels of pain, anxiety, hypertension, and the like. Statistical comparisons in such trials can be made in several ways. Comparison of follow up (post-treatment) scores will give a result such as “at the end of the trial, mean pain scores were 15 mm (95\% confidence interval 10 to 20 mm) lower in the treatment group.” Alternatively a change score can be calculated by subtracting the follow up score from the baseline score, leading to a statement such as “pain reductions were 20 mm (16 to 24 mm) greater on treatment than control.” If the average baseline scores are the same in each group the estimated treatment effect will be the same using these two simple approaches. If the treatment is effective the statistical significance of the treatment effect by the two methods will depend on the correlation between baseline and follow up scores. If the correlation is low using the change score will …},
  langid = {english},
}

@article{wald1945,
  title = {Sequential Tests of Statistical Hypotheses},
  author = {Wald, A.},
  date = {1945},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {16},
  number = {2},
  eprint = {2235829},
  eprinttype = {jstor},
  pages = {117--186},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
}

@article{wulf2016,
  title = {Optimizing Performance through Intrinsic Motivation and Attention for Learning: {{The OPTIMAL}} Theory of Motor Learning},
  shorttitle = {Optimizing Performance through Intrinsic Motivation and Attention for Learning},
  author = {Wulf, Gabriele and Lewthwaite, Rebecca},
  date = {2016-10-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {23},
  number = {5},
  pages = {1382--1414},
  issn = {1531-5320},
  doi = {10.3758/s13423-015-0999-9},
  url = {https://doi.org/10.3758/s13423-015-0999-9},
  urldate = {2022-07-19},
  abstract = {Effective motor performance is important for surviving and thriving, and skilled movement is critical in many activities. Much theorizing over the past few decades has focused on how certain practice conditions affect the processing of task-related information to affect learning. Yet, existing theoretical perspectives do not accommodate significant recent lines of evidence demonstrating motivational and attentional effects on performance and learning. These include research on (a) conditions that enhance expectancies for future performance, (b) variables that influence learners’ autonomy, and (c) an external focus of attention on the intended movement effect. We propose the OPTIMAL (Optimizing Performance through Intrinsic Motivation and Attention for Learning) theory of motor learning. We suggest that motivational and attentional factors contribute to performance and learning by strengthening the coupling of goals to actions. We provide explanations for the performance and learning advantages of these variables on psychological and neuroscientific grounds. We describe a plausible mechanism for expectancy effects rooted in responses of dopamine to the anticipation of positive experience and temporally associated with skill practice. Learner autonomy acts perhaps largely through an enhanced expectancy pathway. Furthermore, we consider the influence of an external focus for the establishment of efficient functional connections across brain networks that subserve skilled movement. We speculate that enhanced expectancies and an external focus propel performers’ cognitive and motor systems in productive “forward” directions and prevent “backsliding” into self- and non-task focused states. Expected success presumably breeds further success and helps consolidate memories. We discuss practical implications and future research directions.},
  langid = {english},
  keywords = {Attentional focus,Dopamine,Motivation,Motor performance,Positive affect,Self-efficacy},
}

@article{wulf2021,
  title = {Translating Thoughts Into Action: Optimizing Motor Performance and Learning Through Brief Motivational and Attentional Influences},
  shorttitle = {Translating {{Thoughts Into Action}}},
  author = {Wulf, Gabriele and Lewthwaite, Rebecca},
  date = {2021-12-01},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {30},
  number = {6},
  pages = {535--541},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1177/09637214211046199},
  url = {https://doi.org/10.1177/09637214211046199},
  urldate = {2022-07-19},
  abstract = {Skilled motor performance is essential in sports, the performing arts, various occupations, and many daily activities. Scientists and practitioners alike are therefore interested in understanding the conditions that influence the performance and learning of movement skills, and how they can be utilized to optimize training. In OPTIMAL theory, three motivational and attentional factors are key: enhanced expectancies for future performance, the performer’s autonomy, and an external focus of attention. We review recent evidence suggesting that each factor contributes independently to strengthen the coupling of goals to actions. This work has implications ranging from fostering more effective skill development in novice learners, to increasing the efficiency of athletes’ and musicians’ performance, and to facilitating the success of patients in regaining functional capabilities.},
  langid = {english},
  keywords = {autonomy,enhanced expectancies,external focus of attention,motor skill,OPTIMAL theory},
}

@article{yantha2022,
  title = {The Recommendation for Learners to Be Provided with Control over Their Feedback Schedule Is Questioned in a Self-Controlled Learning Paradigm},
  author = {Yantha, Zachary D. and McKay, Brad and Ste-Marie, Diane M.},
  date = {2022-04-03},
  journaltitle = {Journal of Sports Sciences},
  volume = {40},
  number = {7},
  pages = {769--782},
  issn = {0264-0414},
  doi = {10.1080/02640414.2021.2015945},
  url = {https://doi.org/10.1080/02640414.2021.2015945},
  urldate = {2022-07-19},
  abstract = {Evidence that self-controlled feedback schedules are more effective for motor learning than yoked or predetermined schedules has been used to forward the recommendation that practitioners should provide choice to learners over when they would like to receive feedback. This recommendation can be questioned because the typical comparison groups in such experimentation do not well represent the applied setting. Consequently, comparison groups that better map onto the applied setting are needed. To this end, three groups learned a golf putting task: (1) self-controlled, (2) traditional-yoked, and (3) a group who were led to believe their KR schedule was being controlled by a golf coach. Participants (N = 60) completed a pre-test, acquisition phase, and delayed post-tests (retention/transfer). No group differences during the post-tests for mean radial error, F(2, 54) = 2.71, p = .075, or bivariate variable error, F(2, 56) = 0.11, p = .896, were found. Thus, the typical self-controlled learning advantage was not observed. Given the failure to replicate self-controlled benefits, combined with the fact there is little research that has directly compared self-controlled feedback schedules to coach-controlled schedules, we argue more evidence is needed before advocating that learners be provided choice over their feedback schedule.},
  keywords = {autonomy,coach-control,knowledge-of-results,Motor learning,self-controlled feedback},
}

@article{wulf2010,
  title={Motor skill learning and performance: a review of influential factors},
  author={Wulf, Gabriele and Shea, Charles and Lewthwaite, Rebecca},
  journal={Medical education},
  volume={44},
  number={1},
  pages={75--84},
  year={2010},
  publisher={Wiley Online Library}
}

@article{mckay2022b,
  title={Low prevalence of a priori power analyses in motor behavior research},
  author={McKay, Brad and Corson, Abbey and Vinh, Mary-Anne and Jeyarajan, Gianna and Tandon, Chitrini and Brooks, Hugh and Hubley, Julie and Carter, Michael J.},
  volume={1},
  number={aop},
  pages={1--14},
  year={2022},
  journal={Journal of Motor Learning and Development},
  publisher={Human Kinetics},
  doi={10.1123/jmld.2022-0042},
  url={https://doi.org/10.1123/jmld.2022-0042}
}

@article{mckay2022c,
  title = {On the reproducibility of power analyses in motor behavior research},
  author = {McKay, Brad and Bacelar, Mariane F. B. and Carter, Michael J.},
  year = {in-press},
  journal={Journal of Motor Learning and Development},
  publisher = {Human Kinetics}
}

@article{stanley2014,
  title={Meta-regression approximations to reduce publication selection bias},
  author={Stanley, Tom D and Doucouliagos, Hristos},
  journal={Research Synthesis Methods},
  volume={5},
  number={1},
  pages={60--78},
  year={2014},
  publisher={Wiley Online Library}
}

@article{hong2021,
  title={Using Monte Carlo experiments to select meta-analytic estimators},
  author={Hong, Sanghyun and Reed, W Robert},
  journal={Research Synthesis Methods},
  volume={12},
  number={2},
  pages={192--215},
  year={2021},
  publisher={Wiley Online Library}
}

@article{lovakov2021,
	title = {Empirically derived guidelines for effect size interpretation in social psychology},
	author = {{Lovakov}, {Andrey} and {Agadullina}, {Elena R.}},
	journal = {European Journal of Social Psychology},
	date = {2021},
	volume = {00},
	pages = {1--20},
	doi = {10.1002/ejsp.2752},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2752},
	note = {{\_}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ejsp.2752},
	langid = {en}
}
